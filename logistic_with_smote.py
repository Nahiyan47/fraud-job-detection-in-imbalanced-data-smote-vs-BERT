# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e53fnzzaAu5fdnhETnIqcpHNRMpyGhZN
"""

# this code is logistic with smote

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score, roc_curve, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
import seaborn as sns
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
import re

# Ensure required NLTK resources are available
nltk.download("vader_lexicon")

# Load dataset
data = pd.read_csv('/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')

# Drop unnecessary columns and fill missing values
text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']
data[text_columns] = data[text_columns].fillna('')

# Combine all textual columns into a single text column
data['text'] = data[text_columns].apply(lambda x: ' '.join(x), axis=1)

# Preprocess text (removing special characters and converting to lowercase)
def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text.lower()

data['clean_text'] = data['text'].apply(preprocess_text)

# Add sentiment analysis features
sia = SentimentIntensityAnalyzer()
data['sentiment'] = data['clean_text'].apply(lambda x: sia.polarity_scores(x)['compound'])

# TF-IDF vectorization with increased max_features
vectorizer = TfidfVectorizer(max_features=200, stop_words='english')
X_tfidf = vectorizer.fit_transform(data['clean_text'])

# Combine TF-IDF features with sentiment scores
X = np.hstack((X_tfidf.toarray(), data['sentiment'].values.reshape(-1, 1)))

# Target variable
y = data['fraudulent']

# Address class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)

# Compute class weights for better handling of imbalance
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = {0: class_weights[0], 1: class_weights[1]}

# Train a Logistic Regression classifier with class weights
clf = LogisticRegression(class_weight=class_weights_dict, random_state=42, max_iter=1000)
clf.fit(X_train, y_train)

# Evaluate the model
y_pred = clf.predict(X_test)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

# Display evaluation metrics for recall and F1 score
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(f"Recall Score: {recall}")
print(f"F1 Score: {f1}")

# Predict probabilities for ROC Curve
y_pred_proba = clf.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Compute AUC score
auc_score = roc_auc_score(y_test, y_pred_proba)
print(f"ROC AUC Score: {auc_score:.2f}")

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})', color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Get TF-IDF feature names and add the sentiment feature name
tfidf_feature_names = vectorizer.get_feature_names_out()
all_feature_names = list(tfidf_feature_names) + ['sentiment']

# Create a DataFrame for feature importances
feature_importances = np.abs(clf.coef_.flatten())  # Logistic regression coefficients as feature importances
important_features = pd.DataFrame({'Feature': all_feature_names, 'Importance': feature_importances})

# Sort by importance and display the top 20 features
important_features = important_features.sort_values(by='Importance', ascending=False).head(20)

# Plot top 20 feature importances
sns.barplot(x='Importance', y='Feature', data=important_features)
plt.title('Top 20 Feature Importances')
plt.show()
